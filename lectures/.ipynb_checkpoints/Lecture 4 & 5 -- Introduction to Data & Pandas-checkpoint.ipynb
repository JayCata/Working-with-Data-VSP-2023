{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b03eb6",
   "metadata": {},
   "source": [
    "# Lecture 4 & 5 -- Introduction to Data & Pandas\n",
    "In this lecture, students will learn about how to work with data and use the most popular data analysis and manipulation library, `pandas`. This is one of a few lectures on `pandas` and related concepts. In this particular lecture, we cover the following topics among others.\n",
    "- Basic Pandas functionality\n",
    "- Pandas data types\n",
    "- Key methods for Pandas data types\n",
    "- Saving and loading data\n",
    "- Data cleaning\n",
    "- Reshaping\n",
    "- Merging\n",
    "- Grouping and Aggregating\n",
    "- Time series data\n",
    "\n",
    "This lecture follows QuantEcon's lecture on [Pandas](https://datascience.quantecon.org/pandas/index.html) more so than previous lectures and uses many of the same examples and code. Note that Pandas has a lot of functionality, and we cannot cover it all in this lecture. This lecture should provide you with a good foundation to learn more on your when the time comes. \n",
    "\n",
    "If you need to look at the detail of a specific function, visit the [Pandas documentaiton](https://pandas.pydata.org/docs/). If you have a question or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd5bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    " # %pip install quandl for installing quandl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff412db",
   "metadata": {},
   "source": [
    "## Importing `pandas`\n",
    "Below we import `pandas` using its common alias `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b36d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import quandl # use this to load dataset later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef296b92",
   "metadata": {},
   "source": [
    "## Series\n",
    "`pandas` has a data type called series. A series is a single column of data. Series, however, allow for custom indices. For instance, below we define a series that contains the winning NBA team for each year from 2010 to 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee7a6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010       Lakers\n",
      "2011    Mavericks\n",
      "2012         Heat\n",
      "2013         Heat\n",
      "2014        Spurs\n",
      "2015     Warriors\n",
      "2016    Cavaliers\n",
      "2017     Warriors\n",
      "2018     Warriors\n",
      "2019      Raptors\n",
      "2020       Lakers\n",
      "2021        Bucks\n",
      "2022     Warriors\n",
      "Name: NBA Finals Champions, dtype: object\n"
     ]
    }
   ],
   "source": [
    "team_names = [\"Lakers\", \"Mavericks\", \"Heat\", \"Heat\", \"Spurs\", \"Warriors\", \"Cavaliers\",\n",
    "         \"Warriors\", \"Warriors\", \"Raptors\", \"Lakers\", \"Bucks\", \"Warriors\"]\n",
    "years = list(range(2010,2023))\n",
    "\n",
    "nba = pd.Series(data = team_names, index = years, name = \"NBA Finals Champions\")\n",
    "print(nba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40ccd3",
   "metadata": {},
   "source": [
    "### Series Indexing\n",
    "Indexing works much like lists except for now, we have to use the indicies we provided to `pd.Series`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf0522",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The winners of the 2012 NBA finals were the {nba[2012]}.\")\n",
    "print(nba[[2012, 2014, 2016]]) # Print a subseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061add73",
   "metadata": {},
   "source": [
    "Like a dictionary, we can reextract a list of the indices and values using `.index` and `.values` respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb94bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nba.index)\n",
    "print(nba.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97abf2b7",
   "metadata": {},
   "source": [
    "### Series Methods\n",
    "Series objects have methods, some of which we demonstrate below. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e4bfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(nba.head(3)) # prints first three entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ad072",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nba.tail())  # prints last five entries by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba.unique() # returns a list of all teams that won at least once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27973a",
   "metadata": {},
   "source": [
    "### Series Plots\n",
    "We can also easily plot series using methods! Below, we define a series of containing the annual US unemployment rate (example from [QuantEcon](https://datascience.quantecon.org/pandas/intro.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fea40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values = [5.6, 5.3, 4.3, 4.2, 5.8, 5.3, 4.6, 7.8, 9.1, 8., 5.7]\n",
    "years = list(range(1995, 2017, 2))\n",
    "unemp = pd.Series(data=values, index=years, name=\"Unemployment\")\n",
    "unemp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd5c742",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "A DataFrame is a table of data -- you can think of a DataFrame as many series stacked side-by-side. This data structure is very similar to the tabular data you might see in an Excel sheet. \n",
    "\n",
    "Like series, DataFrames have row indices, but they also have column labels or names. Having two set of indices allows us to refer to individual columns or rows when needed. \n",
    "\n",
    "Below, we define a DataFrame of unemployment rates for various regions in the US (example from [QuantEcon](https://datascience.quantecon.org/pandas/intro.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205a262",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"NorthEast\": [5.9,  5.6,  4.4,  3.8,  5.8,  4.9,  4.3,  7.1,  8.3,  7.9,  5.7],\n",
    "    \"MidWest\": [4.5,  4.3,  3.6,  4. ,  5.7,  5.7,  4.9,  8.1,  8.7,  7.4,  5.1],\n",
    "    \"South\": [5.3,  5.2,  4.2,  4. ,  5.7,  5.2,  4.3,  7.6,  9.1,  7.4,  5.5],\n",
    "    \"West\": [6.6, 6., 5.2, 4.6, 6.5, 5.5, 4.5, 8.6, 10.7, 8.5, 6.1],\n",
    "    \"National\": [5.6, 5.3, 4.3, 4.2, 5.8, 5.3, 4.6, 7.8, 9.1, 8., 5.7]\n",
    "}\n",
    "years = list(range(1995, 2017, 2))\n",
    "\n",
    "unemp_region = pd.DataFrame(data, index=years)\n",
    "unemp_region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec93ab92",
   "metadata": {},
   "source": [
    "### DataFrame Indexing\n",
    "Indexing dataframes requires use of `.loc` as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ce480",
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_1995 = unemp_region.loc[1995, \"NorthEast\"]\n",
    "print(f\"The 1995 Unemployment Rate for the North East Region is {ne_1995}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_region.loc[[1995, 2005], \"South\"] # Multiple at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844cc34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unemp_region[\"MidWest\"] # extracts whole column, doesn't need to use .loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31a811",
   "metadata": {},
   "source": [
    "### Column Computations\n",
    "We can do computations with columns of a DataFrame. First let's see what type a column of a DataFrame has and try some computations out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd50119",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(unemp_region[\"MidWest\"])\n",
    "# Can divide columns if they are numerical\n",
    "\n",
    "print(unemp_region[\"West\"] / 100) # turn percentage to proportion\n",
    "\n",
    "print(unemp_region[\"West\"] - unemp_region[\"MidWest\"]) # can subtract columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246ec55",
   "metadata": {},
   "source": [
    "### Data Types\n",
    "We've seen that Series and DataFrames can contain numbers or strings. Using `.dtype` on a series of `.dtypes` on a DataFrames tells us the datatype of each column. DataFrames have a few types:\n",
    "- Booleans\n",
    "- Floating point numbers\n",
    "- Integers\n",
    "- Dates\n",
    "- Categorical Data\n",
    "- Everything else (objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4892d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(unemp.dtype)\n",
    "print(nba.dtype) # strings are generic objects in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b424c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(unemp_region.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05c77b",
   "metadata": {},
   "source": [
    "### Creating New Columns\n",
    "If `df` is a dataframe, we can create a new column by simply writing\n",
    "`df[\"New Column Name\"] = new_values` where `new_values` has the same number of elements as `df` has rows. Below, we create an average of all region's unemployment rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_region[\"UnweightedMean\"] = (unemp_region[\"NorthEast\"] +\n",
    "                                  unemp_region[\"MidWest\"] +\n",
    "                                  unemp_region[\"South\"] +\n",
    "                                  unemp_region[\"West\"])/4\n",
    "unemp_region.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a04e5",
   "metadata": {},
   "source": [
    "### Changing Individual Values\n",
    "It is somewhat inefficient to change individual vaues of a DataFrame, but it can be done using `.loc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816a105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unemp_region.loc[1995, \"UnweightedMean\"] = 0.0\n",
    "unemp_region.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efb8e96",
   "metadata": {},
   "source": [
    "## Renaming Columns\n",
    "To rename columns, we firs tmus define a dictionary where the old column names we want to replace are the keys of the dictionary and the new column names are the values. Then, we use the DataFrame method `.rename()`. `.rename` does not replace the column names, it create a copy of the dataframe with the new column names, so you need to assign your dataframe to the output of `.rename()` or use the option `inplace = True`.\n",
    "\n",
    "\n",
    "Below, we rename all region columns to abbreviated names. \n",
    "\n",
    "Then, we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a1be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = {\"NorthEast\": \"NE\",\n",
    "         \"MidWest\": \"MW\",\n",
    "         \"South\": \"S\",\n",
    "         \"West\": \"W\"}\n",
    "unemp_region.rename(columns=names)\n",
    "print(unemp_region.head())\n",
    "unemp_region  = unemp_region.rename(columns=names)\n",
    "print(unemp_region.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f6bed5",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "It would be wildly inefficient if we had to define DataFrames manually, especially when dealing with massive datasets. Luckily, we can load data from our computers or from a URL using the `pd.read_csv()`. below, we load monthly state-level unemployment data from a URL. If we were loading fom our computer, we would replace the url with a local file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e47d1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://datascience.quantecon.org/assets/data/state_unemployment.csv\"\n",
    "unemp_raw = pd.read_csv(url, parse_dates=[\"Date\"])\n",
    "unemp_raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4235a34",
   "metadata": {},
   "source": [
    "`read_csv` will automatically try and determine what datatype each column is. With this particular dataset, Pandas needs a little help, so we let it know the column \"Date\" should be interpreted as a date data type. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb682f",
   "metadata": {},
   "source": [
    "## Quick Data Manipulation\n",
    "We are going to quickly manipulate the data we loaded, so we can illustrate more examples. Do not worry about understanding how the code below works. We will discuss that more later. Let's just talk about how the data looks after we do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe01ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unemp_all = (\n",
    "    unemp_raw\n",
    "    .reset_index()\n",
    "    .pivot_table(index=\"Date\", columns=\"state\", values=\"UnemploymentRate\")\n",
    ")\n",
    "\n",
    "states = [\n",
    "    \"Arizona\", \"California\", \"Florida\", \"Illinois\",\n",
    "    \"Michigan\", \"New York\", \"Texas\", \"Colorado\"\n",
    "]\n",
    "unemp = unemp_all[states]\n",
    "\n",
    "unemp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcbb2f7",
   "metadata": {},
   "source": [
    "## What Changed?\n",
    "We've manipulated the data so every row is a month (month is now the row index) and every column is a state. Each entry is the unemployment rate of that state in the given month.  Since every entry is a number, we can use the plot method on the whole DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a82e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unemp.plot(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57143cc6",
   "metadata": {},
   "source": [
    "## New Indices\n",
    "Now our data is indexed by dates! Let's try and access all unemployment rates on 2015-01-01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unemp.loc[\"01/01/2015\", :])\n",
    "print(unemp.loc[\"01/01/2015\":\"03/01/2015\", :])# we can also view many dates at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e36b2",
   "metadata": {},
   "source": [
    "## DataFrame Aggregations\n",
    "Simply put, aggregation is simply an operation that combines multiple values into a single value. One example of this is an average -- an average takes a collection of numbers (e.g. `[1, 2, 4, 5]`) and returns its average (e.g. 3). \n",
    "\n",
    "Pandas has many built-in aggregations such as:\n",
    "- Mean/Average (`mean`)\n",
    "- Mode (`mode`)\n",
    "- Median (`median`)\n",
    "- Maximum (`max`)\n",
    "\n",
    "among others. \n",
    "\n",
    "For instance, we can see what Colorado's median and maximum unemployment rate was. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1098a2b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "co_unemp = unemp_raw.loc[unemp_raw.state == \"Colorado\",:]\n",
    "co_unemp.UnemploymentRate.median()\n",
    "co_unemp.UnemploymentRate.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04609b",
   "metadata": {},
   "source": [
    "### Custom Aggregations\n",
    "Using a custom function that takes a series as an input and the `.agg()` method, we can create custom aggregations. Below, we creat a function that classifies a state as high or low unemployment based on its average unemployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa72d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_or_low(s):\n",
    "    \"\"\"\n",
    "    This function takes a pandas Series object and returns high\n",
    "    if the mean is above 6.5 and low if the mean is below 6.5\n",
    "    \"\"\"\n",
    "    if s.mean() < 6.5:\n",
    "        out = \"Low\"\n",
    "    else:\n",
    "        out = \"High\"\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b73a59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(unemp.agg(high_or_low)) \n",
    "unemp.agg([min, max, high_or_low]) # Can do multiple aggregate at once -- returns DataFrame not series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f875174",
   "metadata": {},
   "source": [
    "## Transformations \n",
    "Sometimes we want to transform the columns according to a function that also return a series. Some built-in transformations include\n",
    "- Cumulative sum/max/min/product (`cum(sum)`, `cum(max)`, etc.)\n",
    "- Differences (`diff`)\n",
    "- Elementwise addition/subtraction/multiplication/divison (`+`, `-`, `*`, `/`)\n",
    "- Percent change (`pct_change`)\n",
    "- Number of occurence for each distinct value (`value_counts`)\n",
    "- Absolute value (`abs`)\n",
    "\n",
    "For instance, we could calculate the change in unemployment rate from one month to the next for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f84a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75a504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unemp.diff().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a0c6e",
   "metadata": {},
   "source": [
    "## Custom Series Transforms with `.apply()`\n",
    "If we want to transform each column as we did above, but with a custom function, we can do this using the `.apply()` method. \n",
    "\n",
    "To do this, we need to write a function that takes in a series and returns a series. Then we pass the function to the `.apply()` method. Below, we define a function that standardizes each value in a column. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(x):\n",
    "    \"\"\"\n",
    "    Changes the data in a Series to become mean 0 with standard deviation 1\n",
    "    \"\"\"\n",
    "    mu = x.mean()\n",
    "    std = x.std()\n",
    "\n",
    "    return (x - mu)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2dd11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "std_unemp = unemp.apply(standardize_data)\n",
    "std_unemp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb752bcb",
   "metadata": {},
   "source": [
    "## Index Revisited\n",
    "Indices were introduced as being labels for rows, but they also serve other functions. Namely, a given data point will be associated with a specific row index until we as the users explictly break that association. \n",
    "\n",
    "Let's discuss the code below. What do you think will happen when we ad `a + b`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba8c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series({1:5, 2:5, 3:20, 4:14, 10:5})\n",
    "b =pd.Series({1:5, 4:10, 3:10, 6:2})\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af40da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06770b0e",
   "metadata": {},
   "source": [
    "`pandas` knows that when we add two Series together, we want elements with the same index to be added! If a given index does not exist in both Series, it returns `NaN`. This means we don't have to concern ourselves with whether or not data was shuffled around accidentally when performing operations on DataFrames and Series. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19345c09",
   "metadata": {},
   "source": [
    "## Saving Files Formats\n",
    "For this class, we will only use CSVs. Nevertheless, it is important to understand the various file formats that exist. We will review some file formats discussed on [QuantEcon](https://datascience.quantecon.org/pandas/storage_formats.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d4805",
   "metadata": {},
   "source": [
    "## Writing to CSVs\n",
    "Pandas DataFrames have a method called `.to_csv()` that requires a file path for the file. Generally, the file path is relative to the `working directory`.\n",
    "You can think of a working directory as the default directory that your Jupyter notebook operates in. \n",
    "\n",
    "You can check your present working directory by using `pwd()`\n",
    "\n",
    "\n",
    "In Jupyter Open, it will save onto the cloud, but you can download it to you local computer easily. \n",
    "\n",
    "\n",
    "Below, we save our standardized unemployment dataframe to a csv with the title \"stand_unemp.csv\" in the folder `lecture_generated_objects`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82426f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130639b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_unemp.to_csv(\"lecture_generated_objects\\stand_unemp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67980f71",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "After you obtain some data and load it into Python for the first time, it is rarely ready for analysis. Not only do you have to familiarize yourself with the data, what variable it contains, how those variables are coded, etc., but also you also may need to clean the data. **Data cleaning**  refers to basically anything you do after you have loaded the data into Python but before you analyze it. This includes but is not limited to:\n",
    "- correcting typos or incorrectly coded data points\n",
    "- data may not be of the right type\n",
    "- throwing out or filling in missing data\n",
    "- joining datasets across many files into one DataFrame\n",
    "- removing outliers\n",
    "- reshaping data into the needed form\n",
    "- taking a subsample of the data\n",
    "\n",
    "\n",
    "In that sense, you could say the remainder of what we learn about Pandas is about data cleaning. Whatever you call it, `pandas` includes tools to deal with all of these issues data presents us. Consider the following mock dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4c755",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "df = pd.DataFrame({\"numbers\": [\"#23\", \"#24\", \"#18\", \"#14\", \"#12\", \"#10\", \"#35\"],\n",
    "                   \"nums\": [\"23\", \"24\", \"18\", \"14\", math.nan, \"XYZ\", \"35\"],\n",
    "                   \"colors\": [\"green\", \"red\", \"yellow\", \"orange\", \"purple\", \"blue\", \"pink\"],\n",
    "                   \"other_column\": [0, 1, 0, 2, 1, 0, 2]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c9e337",
   "metadata": {},
   "source": [
    "## Type Issues\n",
    "When working with data, the columns may not be of the `dtype` we want. This can stop us from performing certain operations on the data. Below, we try and use the aggregator method `.mean()` on the column `numbers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aaae7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment this line and see the error\n",
    "#df.numbers.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55110f",
   "metadata": {},
   "source": [
    "As the error tells us, the numbers column is actually a string, so we need to convert the numbers column to a numeric data type. First, however, we must get rid of the the # symbol.\n",
    "\n",
    "We could do this by looping through each element of the column and use the string method `.replace()`, but that is highly inefficient. Instead, we can apply string methods (not just `.replace()`) directly to pandas columns that contain strings to create a new column!\n",
    "\n",
    "To make it so our new column is a numeric `dtype`, we simply use the pandas function `pd.to_numeric()`. Alternatively, we can use the Series method `astype()` which take a Python type as an argument and tries to convert the column to that type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the pound symbol\n",
    "df[\"numbers_str_1\"] = pd.to_numeric(df[\"numbers\"].str.replace(\"#\", \"\"))\n",
    "print(df[\"numbers_str_1\"].mean())\n",
    "\n",
    "df[\"numbers_str_2\"] = (df[\"numbers\"].str.replace(\"#\", \"\")).astype(float)\n",
    "print(df[\"numbers_str_2\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599767e4",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "**Missing data** is an important consideration when performing an analysis. In its most literal form, missing data is best thought of in the context of tables. A table hass missing data when one or more cells in that table has no  value in it or a value that is interpreted as missing. In pandas, this missing data is represented by the `NaN` (not a number) symbol. \n",
    "\n",
    "\n",
    "Below, we illustrate some strange behavior that NaNs exhibit and the DataFrame method is.null() which can help you detect and consequently, deal with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b584b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaNs have some weird behaviors that require specific functions\n",
    "print(math.nan == math.nan) # this won't work for detecing nan values\n",
    "math.isnan(math.nan) # have to use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3b65e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.isnull()) # see which cells are missing\n",
    "print(df.isnull().any()) # see if there is a single missing cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf03293",
   "metadata": {},
   "source": [
    "We can also detect if individual rows or columns have missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().any(axis=0))\n",
    "print()\n",
    "print(df.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb07a6",
   "metadata": {},
   "source": [
    "## What can we do with missing data?\n",
    "- Throw out missing data (`.dropna` method)\n",
    "- Fill in the missing data (`.fillna` method)\n",
    "- Model missingness (complicated and beyond the scope of this course) \n",
    "\n",
    "Let's see what `.dropna()` and `.fillna()` do and discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba9a64b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.dropna())\n",
    "print(df.fillna(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dca9df",
   "metadata": {},
   "source": [
    "## \"Tidy\" Data\n",
    "- Concept to help us understand how we should reshape our data\n",
    "- The concept of tidy data is well-summarized by the following quote from Hadley Wickham's \"[Tidy Data](https://www.jstatsoft.org/index.php/jss/article/view/v059i10/v59i10.pdf)\":\n",
    ">A dataset is messy or tidy depending on how rows, columns and tables are matched with observations, variables, and types. In tidy data: \n",
    ">1. Each variable forms a column.\n",
    ">2. Each observation forms a row.\n",
    ">3. Each type of observational unit forms a table\n",
    "- This view usually leads to the question, what is an observation in this context? (e.g year, country, individual, firm, year-country etc.)\n",
    "- This observation-level identifier (e.g. 1993, France, John Smith, Amazon, France-1993, etc.) can serve as your `DataFrame` index as it should be unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12c711",
   "metadata": {},
   "source": [
    "## Reshaping Data\n",
    "Sometimes, data is not in the shape you need it to be in for analysis. To illustrate this, we consider a dataset on a few basketball players and their points scored over various seasons. What is the unit of observation in the `bball` DataFrame? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121895a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://datascience.quantecon.org/assets/data/bball.csv\"\n",
    "bball = pd.read_csv(url)\n",
    "\n",
    "bball"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f56c1d",
   "metadata": {},
   "source": [
    "## Long vs. Wide Tabular Data\n",
    "We can make this table longer (more rows and fewer columns) or wider (fewer rows and more columns) depending on our needs. \n",
    "\n",
    "### Wide to Long\n",
    "If we want to make data longer, there are three potential functions/methods you can use: `.melt()`, `.stack()`, and `wide_to_long()`. \n",
    "\n",
    "### Long to Wide\n",
    "When we want to make our data wider, we can use `.unstack()`,`.pivot()`, and `.pivot_table()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d8223",
   "metadata": {},
   "source": [
    "## Wide to Long with `.melt()`\n",
    "Below, we use `.melt()` to go from wide to long. The areguments we feed to `id_vars` are the variables we want to keep as variables. After observing what this function, what is an observation now? What did `.melt()` do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e021a49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bball_long = bball.melt(id_vars=[\"Year\", \"Player\", \"Team\", \"TeamName\"])\n",
    "bball_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2727a",
   "metadata": {},
   "source": [
    "## Long to Wide with `.pivot()` and `.pivot_table()`\n",
    "Related to pivot tables you may have seen in excel. Both help us go from long to wide. `.pivot_table()` is a generalization of `.pivot()`, so we will focus on it. \n",
    "\n",
    "### `pivot_table()`\n",
    "- Choose columns the index\n",
    "- Choose columns as variables\n",
    "- One column serves as the values\n",
    "- Duplicates are aggregated -- can specify how\n",
    "\n",
    "Easiest to understand with an example. Think of whch of these examples will need some form of aggregation and which will not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea21e57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here we keep the team and year as our index and move the players to the columns. \n",
    "bball.pivot_table(index=[\"Year\", \"Team\"], columns=\"Player\", values=\"Pts\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19685ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Alternatively, we can put the team names as the columns with players\n",
    "bball.pivot_table(index=\"Year\", columns=[\"Player\", \"Team\"], values=\"Pts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bcb4e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This requires aggregation. Why? What is used by default?\n",
    "bball_pivoted = bball.pivot_table(index=\"Year\", columns=\"Player\", values=\"Pts\")\n",
    "bball_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a1ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Can change aggregation type\n",
    "bball.pivot_table(index=\"Year\", columns=\"Player\", values=\"Pts\", aggfunc=max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b7421",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can view how many duplicates each year-player combinations has\n",
    "bball.pivot_table(index=\"Year\", columns=\"Player\", values=\"Pts\", aggfunc=len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fab1ea",
   "metadata": {},
   "source": [
    "We can actually take our long dataframe earlier and use `.pivot_table()` to make it wider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eac1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bball_wide = bball_long.pivot_table(\n",
    "    index=\"Year\",\n",
    "    columns=[\"Player\", \"variable\", \"Team\"],\n",
    "    values=\"value\"\n",
    ")\n",
    "bball_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc804c9",
   "metadata": {},
   "source": [
    "### Aside: Indexing Revisited\n",
    "Before proceeding with `stack()` and `unstack()`, we first must learn three more methods, `.set_index()`, `.reset_index()`, and `.T` (transpose). First, we notice that a player-year uniquely determine a row, so we can set the index as a player-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518558d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bball2 = bball.set_index([\"Player\", \"Year\"])\n",
    "bball2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2324d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transpose switches columns and rows\n",
    "bball3 = bball2.T\n",
    "bball3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe7745",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Demonstrate .reset_index()\n",
    "bball3.reset_index() # Previous index gets saved as a column call index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75fde5c",
   "metadata": {},
   "source": [
    "## Wide to Long with `.stack()`\n",
    "Below, we demonstrate some uses of `.stack()` on `bball_wide`. In the example below, notice that `.stack()` moves the top column by detault. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79bd24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# before\n",
    "bball_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880bf034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# after\n",
    "bball_wide.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3754098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This allows us to easily get player level means for each stastic\n",
    "player_stats = bball_wide.stack().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f3abd",
   "metadata": {},
   "source": [
    "### Choose your Level\n",
    "What if, however, we want to move the players into the index instead of the team? To do this, we simply specify which level we want to move using the level argument. This allows us to easily get team averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f775e0a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bball_wide.stack(level=\"Player\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a0fdb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bball_wide.stack(level=\"Player\").mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ac9f6",
   "metadata": {},
   "source": [
    "### Multiple Levels\n",
    "We can also stack multiple levels simultaneously by specifying many levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836e807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bball_wide.stack(level=[\"Player\", \"Team\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b8337b",
   "metadata": {},
   "source": [
    "## Long to Wide with `.unstack()`\n",
    "Recall the `player_stats` series we defined earlier. Using `.unstack()`, we can turn this series back into a DataFrame. This form makes it easy to make a bar plot by player or by statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed3f14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "player_stats.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88873fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "player_stats.unstack().plot.bar()\n",
    "player_stats.unstack(level = \"Player\").plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc98c6",
   "metadata": {},
   "source": [
    "## Merging Data\n",
    "Frequently, we want to perform analysis on data that comes from two or more different sources or are spread across many files. For example, we may be interested in the prices of a company's stock and its earnings but our data might be spread out across many files.\n",
    "- Different files for different years\n",
    "- Different files for different companies\n",
    "- Different files for stock prices and earnings\n",
    "\n",
    "Depending on our unit of observation, these different situations might require different ways of **merging data** -- that is getting all of the data into a single DataFrame.\n",
    "\n",
    "The three main ways to merge data in pandas are:\n",
    "1. `pd.concat([df1, df2,...])`\n",
    "2. `pd.merge(df1, df2)`\n",
    "3. `df1.join(df2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Datasets Used for Illustration\n",
    "# from World Development Indicators (WDI). Units trillions of 2010 USD\n",
    "url = \"https://datascience.quantecon.org/assets/data/wdi_data.csv\"\n",
    "wdi = pd.read_csv(url).set_index([\"country\", \"year\"])\n",
    "wdi.info()\n",
    "\n",
    "wdi2017 = wdi.xs(2017, level=\"year\")\n",
    "wdi2017\n",
    "\n",
    "# Data from https://www.nationmaster.com/country-info/stats/Geography/Land-area/Square-miles\n",
    "# units -- millions of square miles\n",
    "sq_miles = pd.Series({\n",
    "   \"United States\": 3.8,\n",
    "   \"Canada\": 3.8,\n",
    "   \"Germany\": 0.137,\n",
    "   \"China\": 3.7,\n",
    "   \"Brazil\": 3.3\n",
    "}, name=\"sq_miles\").to_frame()\n",
    "sq_miles.index.name = \"country\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82951333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View datasets so we can compare later\n",
    "wdi2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f55e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View datasets so we can compare later\n",
    "sq_miles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26898f0",
   "metadata": {},
   "source": [
    "### `pd.concat()`\n",
    "\"concat\" stands for concatenation. Simply put, concatenating DataFrames entails stacking them on top of eachother or side-by-side. This,however, is done intelligently though as indices must match. The argument `axis` allows user to specify which way Python should stack the DataFrames. \n",
    "\n",
    "We show two concatenations below. Which one looks more \"tidy?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d245a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The two dataframes have no columns in behavipr\n",
    "pd.concat([wdi2017, sq_miles], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5349700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# China and Brazil are missing from wdi2017\n",
    "pd.concat([wdi2017, sq_miles], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can obtain per square mile statistics now! Remember Exports is in trillions and sq_miles is in millions\n",
    "temp = pd.concat([wdi2017, sq_miles], axis=1)\n",
    "temp[\"Exports\"] *(1_000_000) / (temp[\"sq_miles\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff64a4f",
   "metadata": {},
   "source": [
    "### `pd.merge()` or `df.merge()`\n",
    "Brings columns from one DataFrame into another, but instead of just combining them using an index, it uses one or more keys (variables found in both datasets) to combine the data.\n",
    "\n",
    "### `df.join()`\n",
    "This is a less general version of merge that is a DataFrames method instead of a function.\n",
    "\n",
    "Merging & joining are not straightforward concepts, so let's start with some examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61154a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(wdi2017, sq_miles, on=\"country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e54a36",
   "metadata": {},
   "source": [
    "### What do we see?\n",
    "- China, Brazil, and the UK were excluded. What did those countries have in common (with regards to the data)?\n",
    "- Besides that, similar to `pd.concat([wdi2017, sq_miles], axis = 1)`\n",
    "\n",
    "The power of merging is more obvious when we merge on a variable that is not unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67c96d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load wdi data for 2016 and 2017\n",
    "wdi2016_17 = wdi.loc[pd.IndexSlice[:, [2016, 2017]],: ]\n",
    "wdi2016_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8effc33a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge `wdi2016_17` data with `sq_miles`\n",
    "pd.merge(wdi2016_17, sq_miles, on=\"country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd68eb6",
   "metadata": {},
   "source": [
    "### What do we see?\n",
    "- China, Brazil, and the UK were excluded again.\n",
    "- We lost the `year` variable. \n",
    "- Each country has the `sq_miles`value repeated twice -- once for 2016 and once for 2017\n",
    "\n",
    "To recover the year, simply reset the index, so that it becomes a column instead. If you want, you can set the index back again using `.set_index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3445b62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = pd.merge(wdi2016_17.reset_index(), sq_miles, on=\"country\").set_index([\"country\", \"year\"])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f03d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1_000_000 * temp[\"Consumption\"] / temp[\"sq_miles\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392d651",
   "metadata": {},
   "source": [
    "## Multiple Column Merges\n",
    "What if we have multiple datasets with country-observation years that we want to merge together? Luckily, `pd.merge()` makes this easy. Below, we load another datset that contains the 2000-2017 populations of the folowing countries:\n",
    "- Canada\n",
    "- Germany\n",
    "- United States\n",
    "- United Kingdom\n",
    "\n",
    "Then, we merge this new dataframe with `wdi2016_17`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7da20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Loading a dataset on population\n",
    "# from WDI. Units millions of people\n",
    "pop_url = \"https://datascience.quantecon.org/assets/data/wdi_population.csv\"\n",
    "pop = pd.read_csv(pop_url).set_index([\"country\", \"year\"])\n",
    "pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd301e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wdi_pop = pd.merge(wdi2016_17, pop, on=[\"country\", \"year\"])\n",
    "wdi_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb4ce04",
   "metadata": {},
   "source": [
    "### What do we see?\n",
    "- China, Brazil, and the UK were excluded again.\n",
    "- We didn't lose the the `year` variable even though it was the index because we it was a variable we merged on! \n",
    "- Population matches as we would expect.\n",
    "\n",
    "\n",
    "Now we can use this to dataset to get per-capita measurements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc012ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "1_000_000 * wdi_pop[\"GDP\"] / wdi_pop[\"Population\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3708a2",
   "metadata": {},
   "source": [
    "## Other `merge` Arguments\n",
    "`pd.merge()` has optional arguments we have not discussed. To use these, you should understand the concept of `left` and `right`. Namely, the DataFrame that serves as the first argument in `merge()` is the `left` DataFrame. The second DataFrame argument is the `right` DataFrame. \n",
    "\n",
    "**Example:** `pd.merge(df_left, df_right, on = variable)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174f4bc",
   "metadata": {},
   "source": [
    "### `on`, \n",
    "This is technically an optional argument. If it is not specified, all column names (**not indices!**) that appear in both `left` and `right` DataFrames will be used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fb25b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This will not work if we do not .reset_index() \n",
    "pd.merge(wdi2016_17.reset_index(), pop.reset_index()).set_index([\"country\", \"year\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cfc384",
   "metadata": {},
   "source": [
    "### `left_on` and `right_on`\n",
    "Use this when the columns you want to match on are called something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DataFraem with different index name\n",
    "cont_df = pd.Series({\"Canada\":\"North America\", \n",
    "                  \"Germany\":\"Europe\",\n",
    "                  \"China\":\"Asia\",\n",
    "                  \"Brazil\":\"South America\",\n",
    "                  \"Kenya\":\"Africa\"}).to_frame()\n",
    "cont_df.index.name = \"country or something else\"\n",
    "cont_df = cont_df.rename(columns = {0:\"Continent\"})\n",
    "cont_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5dd2c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reset index on the variable we want to keep as they will be identical\n",
    "pd.merge(sq_miles.reset_index(), cont_df, left_on = \"country\", right_on = \"country or something else\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df6317",
   "metadata": {},
   "source": [
    "### `left_index` and `right_index`\n",
    "If we want to merge on the index on our `left` or `right` DataFrames, we can set  `left_index` or  `right_index` equal to `True` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78084322",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(sq_miles.reset_index(), cont_df, left_on = \"country\", right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0abd503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(wdi2017, cont_df, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad902b",
   "metadata": {},
   "source": [
    "### `how` \n",
    "This far, only keys (countries and years) that have been in both the `left` and `right` dataset appear in the merged dataset. This behavior is caused by the default value of `how`. What if we want these keys to stick around even if they don't exist in the other dataset?\n",
    "\n",
    "This is one of the two most difficult Pandas concepts we will cover and requires keeping track of which DataFrame is `left` and which one is `right`.  \n",
    "\n",
    "`how` can take four arguments:\n",
    "- \"left\"\n",
    "- \"right\"\n",
    "- \"inner\"\n",
    "- \"outer\"\n",
    "\n",
    "Below, is a helpful image from [QuantEcon](https://datascience.quantecon.org/pandas/merge.html) that shows what each argument does. Can you guess which is the default one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c38cd1",
   "metadata": {},
   "source": [
    "<img src=\"https://datascience.quantecon.org/_images/merge_venns.png\" alt=\"Alternative text\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1145dd",
   "metadata": {},
   "source": [
    "### Argument descriptions\n",
    "- \"left\" uses all keys from `left`, regardless of their presence in `right`\n",
    "- \"right\" uses all keys from `right`, regardless of their presence in `left`\n",
    "- \"inner\" uses only keys that appear in both `left` and `right`\n",
    "- \"outer\" uses all keys from both `left` and `right`\n",
    "\n",
    "Let's see some example to solidify our understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd7002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left\n",
    "pd.merge(cont_df, wdi2017, left_index = True, right_index = True, how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1735719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# right\n",
    "pd.merge(cont_df, wdi2017, left_index = True, right_index = True, how = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfcfdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner\n",
    "pd.merge(cont_df, wdi2017, left_index = True, right_index = True, how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a88cf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# outer\n",
    "pd.merge(cont_df, wdi2017, left_index = True, right_index = True, how = \"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b61dd1",
   "metadata": {},
   "source": [
    "## Aggregation with GroupBy\n",
    "Frequently, we want to get aggregate statistics (likes averages, modes, and medians) on a group level. For instance, we might want ot use data on individuals' heights and ages to see how average height varies by age. \n",
    "\n",
    "`.groupby()` is an essential tool for such aggregation and is probably the most difficult Pandas concept we will learn. Using `.groupby()`, we can divide our data into groups and use aggregators to get statistics on the group-level.\n",
    "\n",
    "## Three Steps\n",
    "This process can broken down into three steps:\n",
    "- **Split** the data into groups. These groups will be determined by the values of one or more columns.\n",
    "- For each group, **apply** the same function or process (e.g. mean, median, etc.)\n",
    "- Take the output of that function or process for each group and **combine** them into a single DataFrame where group identifiers serve as the index. \n",
    "\n",
    "To begin, we will demonstrate `.groupby()` with a toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame({\n",
    "    \"A\" : [1, 1, 1, 2, 2, 2, 2, 1],\n",
    "    \"B\" : [\"Green\", \"Red\", \"Green\", \"Green\", \"Blue\", \"Red\", \"Green\", \"Red\"],\n",
    "    \"C\":  [1.0, 2.0, 3.0, math.nan, 5, math.nan, 2.2, 3.4],\n",
    "    \"D\" :  [1, 3, 5, 7, 5, 3, 1, 3]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First group by a\n",
    "gbA = df.groupby(\"A\")\n",
    "type(gbA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8113c2b",
   "metadata": {},
   "source": [
    "### `.get_group()`\n",
    "Note the type is a DataFrameGroupBy. This type has a method `.get_group()` that allows us to view keys by supplying it the key of the group we want to see. See if you can guess what the code will display before we run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4188a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(gbA.get_group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7512ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "(gbA.get_group(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da9b08",
   "metadata": {},
   "source": [
    "### Multiple Column GroupBy\n",
    "Much like we can merge two DataFrames on multiple columns, we can group a DataFrame by multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc8c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbAB = df.groupby([\"A\", \"B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check which groups exist and which indices can be found in each group\n",
    "gbAB.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5badd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Indices are now tuples\n",
    "gbAB.get_group((1, \"Green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4afa46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbAB.get_group((2, \"Blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07aca9",
   "metadata": {},
   "source": [
    "## Aggregators\n",
    "Once we have a GroupBy object, we have completed the first step of splitting the data into two groups. We can then apply aggregator methods to our data set which simultaneously applies the functions to each groups and combines the results into a DataFrame. \n",
    "\n",
    "Looking at the below, how did the `.sum()` aggregator deal with the `NaN` values? \n",
    "How did the `.count()` function deal with with NaNs values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b71594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print original df for comparison\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ebd5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sum all columns (not indices!)\n",
    "gbAB.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced1e38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we how many observations or rows are in each group\n",
    "gbAB.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e52e0",
   "metadata": {},
   "source": [
    "### Custom Aggregators for GroupBy\n",
    "Custom Aggregators also work on GroupBy objects. Simply define a function that takes a DataFrame or Series as an input and returns a Series or a single value. As before, we feed this function to the `.agg()` method.  Below, we define a function that returns the number of missing data points in each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that takes Data Frame\n",
    "def num_missing(df):\n",
    "    return df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7abbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# .agg(num_missing) applies num_missing to each group individually then combines the results\n",
    "gbAB.agg(num_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14356e71",
   "metadata": {},
   "source": [
    "### Custom Transforms for GroupBy\n",
    "We can also apply custom transforms to GroupBy objects by using the `.apply()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2086a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that returns rows where column B is equal to \"Green\" of a Data Frame\n",
    "def find_green_rows(df):\n",
    "    return df.loc[df[\"B\"] == \"Green\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4189258",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gbAB.apply(find_green_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743ece53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the index changed -- what happened? \n",
    "gbAB.apply(find_green_rows).index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55061517",
   "metadata": {},
   "source": [
    "## Working with Date & Time Data\n",
    "Sometimes, we will have a date or date and time data that we want to work with. Sometimes, tha tentails extracting some component of the date (e.g. month or day of the week) to save as its own variable. Other times, our dates may be stored as strings or a series of numbers, but we want them to be handled as dates (e.g. what date is one day from a given date, was the day before a Monday, how much time elapsed between two datetimes, etc.)\n",
    "\n",
    "`datetime` objects and their associated methods help us deal with all of these scenarios and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead97556",
   "metadata": {},
   "source": [
    "### Parsing Dates from Strings\n",
    "First, we demonstrate how to convert a string containing a date into a `datetime` object. Like other type conversions, we simply use `pd.to_datetime`. It is important that the string is formatted a certain way, so that its contents can be interpreted as a `datetime`.\n",
    "\n",
    "After running the cell below, observe what happens. What happens when we do not provide a time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date string to datetime\n",
    "christmas_str = \"2017-12-25\"\n",
    "christmas_date = pd.to_datetime(christmas_str)\n",
    "christmas_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8597637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date and time string to datetime\n",
    "christmas_time_str = \"2017-12-25, 13:34:34\"\n",
    "christmas_time = pd.to_datetime(christmas_time_str)\n",
    "christmas_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a63b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of dates\n",
    "kwanza_dates = pd.to_datetime([\"2023-12-26\", \"2023-12-27\", \"2023-12-28\", \"2023-12-29\", \"2023-12-30\", \"2023-12-31\", \"2024-01-01\"])\n",
    "kwanza_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698df7c9",
   "metadata": {},
   "source": [
    "### Flexible Date Formats\n",
    "As you can see in the cell directly below, `pd.to_datetime` can parse many date formats. Uncomment the cell below that one, however, and you will see it cannot handle everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c64a45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for date in [\"December 25, 2017\", \"Dec. 25, 2017\",\n",
    "             \"Monday, Dec. 25, 2017\", \"25 Dec. 2017\", \"25th Dec. 2017\"]:\n",
    "    print(\"pandas interprets {} as {}\".format(date, pd.to_datetime(date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d736ec8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We create a very weird date time string\n",
    "weird_time = \"Second:45, Minute:32, Hour:21, Day:3, Month:3, Year:2032\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below and see it fail\n",
    "# pd.to_datetime(weird_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ee073",
   "metadata": {},
   "source": [
    "### Specify the Format for String Conversion\n",
    "Luckily, we can still deal with dates that are formatted like `weird_time` by communicating the pattern to `pd.to_datetime()` using another string. \n",
    "\n",
    "Below, we define such a string and feed it to the argument `format`. Can you figure out what the % symbols follows by a letter below signify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5388eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_pattern_str = \"Second:%S, Minute:%M, Hour:%H, Day:%d, Month:%m, Year:%Y\"\n",
    "pd.to_datetime(weird_time, format = weird_pattern_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a5c35",
   "metadata": {},
   "source": [
    "### Format `datetime` as string\n",
    "We may also want to format `datetime` as a string or extract a certain value from the `datetime` and turn it into a number. We can t dates as strings using the `.strftime()` method.\n",
    "\n",
    "Run the cell below and observe the output. What do you thing `%B` does? How about `%e`? \n",
    "\n",
    "[here](https://strftime.org/) is a usefule reference for some of the possible `.strftime()` formatting options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de630cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Kwanza starts on \" + kwanza_dates[0].strftime(\"%B %d\") + \"th and ends on \" + kwanza_dates[len(kwanza_dates)-1].strftime(\"%B%e\") + \"st.\"\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2571d",
   "metadata": {},
   "source": [
    "### Date Properties\n",
    "Sometimes, we may have `datetime` variables and we want to get numeric values (e.g. the hour, day of the month, year, etc.). Below, we demonstrate how to access some of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.to_datetime(\"2012-11-15\").year)\n",
    "print(pd.to_datetime(\"2012-11-15\").dayofweek)\n",
    "print(pd.to_datetime(\"2012-11-15\").day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83338d",
   "metadata": {},
   "source": [
    "## Dates as Indices\n",
    "In a previous example, we have observations that were country-year and country-year pairs defined our observations uniquely. Using a `DateTimeIndex` instead of a numeric index give us access to many powerful tools. Below, we load data on daily BitCoin to US dollar (USD) exchange rates from 2014 onwards. \n",
    "\n",
    "Note how our index is a `DateTimeIndex`! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b38376",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "btc_usd = quandl.get(\"BCHARTS/BITSTAMPUSD\", start_date = pd.to_datetime(\"01-01-2014\"))\n",
    "btc_usd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441573ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "btc_usd.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d02b5",
   "metadata": {},
   "source": [
    "### Flexibilie Indexing Syntax\n",
    "Normally, we can reference a row only by specifically referencing its index. If we want to select many indices, we can use slices or other collections to select them. \n",
    "\n",
    "With dates, however, we have a different way to select many rows at once that leverages the same flexibility of `pd.to_datetime()`. For example, below we select all prices on August 1st of every year with a single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801eb574",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "btc_usd.loc[\"2019\"] # Select BTC prices in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed6b121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select end of month price data \n",
    "btc_usd.loc[btc_usd.index.is_month_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09adbbbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select all prices in October 2018\n",
    "btc_usd.loc[\"October 2018\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a520b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Can specify a range od ates\n",
    "btc_usd.loc[\"October 2018\":\" December 2019\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ddaea",
   "metadata": {},
   "source": [
    "### Date Index Properties\n",
    "Earlier, we demonstrated how to extract individual numerical elements from a date (year, day of the week, etc.) We can do the same thing with a `DateTimeIndex` object.  These operations return an Int64Index object. If we are operating on a column of `DateTime` objects, then we have to add an extra `.dt` betweent the object and the commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7cadd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Index example\n",
    "print(type(btc_usd.index.year))\n",
    "btc_usd.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4b667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "btc_date_column = btc_usd.reset_index()\n",
    "btc_date_column[\"Date\"].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4236dec8",
   "metadata": {},
   "source": [
    "## Leads and Lags Using `df.shift()`\n",
    "When dealing with what's known as time series data, we sometimes want to compare data from one datetime with another datetime. For example, we might be interested in calculating Bitcoin's price change from one day to the next. To do this, we need to associate prices from one day (e.g May 1st, 2015) with prices from the next day (e.g. May 2nd, 2015). \n",
    "\n",
    "We can do this easily using `df.shift(n)` where `n=1`. Think about why the first rows is all `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c202cb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Before df.shift()\n",
    "btc_usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c201c69a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# after df.shift()\n",
    "btc_usd.shift()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f059b719",
   "metadata": {},
   "source": [
    "Now, we can take advantage of Python's indexing to calculate the percentange change in all of the column variables from one day to the next! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c25538",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "((btc_usd - btc_usd.shift()) / btc_usd.shift()).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec5bae",
   "metadata": {},
   "source": [
    "### Shifting by Different Amounts\n",
    "If we wanted two day price changes we can chose `n=2`. First, let's see what shifting by 2 does. Why are the top two columns now all `NaN`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e901f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "btc_usd.shift(2).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec789402",
   "metadata": {},
   "source": [
    "So far positive numbers have been giving us what are called **lags**. That is, we have been creating associations between a date and data from an **earlier** date. **Leads** are the opposite. In this case, we associate a date with data from a **later** date. To get leads, we use negative numbers.\n",
    "\n",
    "Below, we print the last three rows of `btc_usd.shift(-2)`. What do you expect to see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eaf39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_usd.shift(-2).tail(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
